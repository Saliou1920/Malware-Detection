# -*- coding: utf-8 -*-
"""A00197239_Saliou_Projet.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Snl2r55ZuJvO5xIyaiMk0-Jd6JGG0U2N
"""

!unzip /content/drive/MyDrive/malevis_train_val_224x224.zip
#!unzip /content/drive/'My Drive'/ImageDataset.zip

import keras
import tensorflow
from keras.models import Sequential, Input, Model
from keras.layers import Dense, Dropout, Flatten, Activation, GlobalAveragePooling2D
from keras.layers import Conv2D, MaxPooling2D, AveragePooling2D
from keras.layers.normalization import BatchNormalization
from tensorflow.keras.optimizers import Adam
from tensorflow.keras import optimizers
from keras.preprocessing.image import ImageDataGenerator
from sklearn.model_selection import train_test_split

num_classes = 26

#Generating DataSet
#path_root = "/content/ImageDataset"
path_root = "/content/malevis_train_val_224x224/train"
batches = ImageDataGenerator().flow_from_directory(directory=path_root, target_size=(224,224), batch_size=10000)
imgs, labels = next(batches)

#Split into train and test
X_train, X_test, y_train, y_test = train_test_split(imgs/255.,labels, test_size=0.2)



"""#### Import EfficientNet B0"""

!pip install -U efficientnet

import efficientnet.keras as efn

base_model = efn.EfficientNetB0(input_shape = (224, 224, 3), include_top = False, weights = 'imagenet')

# we freeze the the first 220 layers for fine funing

for layer in base_model.layers[:220]:
    layer.trainable = False

"""### Model"""

from keras import layers
x = base_model.output
x = Flatten()(x)
x = Dense(1024, activation="relu")(x)
x = Dropout(0.5)(x)
x = layers.Dense(num_classes, activation='softmax')(x)
model_final = keras.models.Model(base_model.input, x)

model_final.summary()

# install keras_metrics to use f1 score metric
!pip install keras_metrics

import keras_metrics
opt = Adam(lr=0.000001)
#opt = Adam(lr=0.0000001)
#opt = optimizers.RMSprop(lr=0.000001)
model_final.compile(loss='categorical_crossentropy', optimizer = opt, metrics=['accuracy', keras_metrics.precision(), keras_metrics.binary_f1_score(), keras_metrics.recall()])

model_final.summary()

"""### balancer les donnees"""

import numpy as np
y_train_new = np.argmax(y_train, axis=1)

from sklearn.utils import class_weight
from keras.callbacks import EarlyStopping
y_train_new = np.argmax(y_train, axis=1)
#Deal with unbalanced Data
class_weights = class_weight.compute_class_weight('balanced', np.unique(y_train_new),y_train_new)
class_weights = dict(enumerate(class_weights))
class_weights

history = model_final.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=20, class_weight=class_weights)



scores = model_final.evaluate(X_test, y_test)

history = model_final.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=20)

scores = model_final.evaluate(X_test, y_test)

"""## Les courebes loss"""

# Affiche de la courbe de l'accuracy et de la validation
import matplotlib.pyplot as plt

# Affichage des courbes d'accuracy, validation loss
epochs = range(len(history.history['accuracy']))
plt.plot(epochs, history.history['loss'], 'b', label='loss')
plt.plot(epochs, history.history['val_loss'], 'r', label='Val loss')
plt.title("Courbe d'accuracy de l'entrainement et de la validation loss ")
plt.legend()

plt.show()

# Affiche de la courbe de l'accuracy et de la validation

plt.plot(epochs, history.history['accuracy'], 'b', label='accuray')
plt.plot(epochs, history.history['val_accuracy'], 'r', label='Val acc')
plt.title("Courbe d'accuracy de l'entrainement et de la validation ")
plt.legend()
plt.figure()

"""## Predictions"""

import numpy as np
import pandas as pd

#y_pred = model.predict_classes(X_test, verbose=0)
y_pred = np.argmax(model_final.predict(X_test), axis=-1)

y_pred

y_test2 = np.argmax(y_test, axis=1)

from sklearn import metrics
c_matrix = metrics.confusion_matrix(y_test2, y_pred)

c_matrix

class_names= batches.class_indices.keys()



#retourne tous les labels de test
 import pandas as pd
# tableau a deux dimension de la matrice de confusion
 df_cm = pd.DataFrame(c_matrix, index=class_names, columns=class_names)
 df_cm



"""## Matrice de confusion"""

import seaborn as sns

plt.figure(figsize = (20,7))

sns.heatmap(df_cm, annot=True, fmt="d")

"""# Classification binaire Malware vs Benign"""

!unzip /content/drive/'My Drive'/dataset.zip

import keras
from keras.models import Sequential, Input, Model
from keras.layers import Dense, Dropout, Flatten, Activation, GlobalAveragePooling2D
from keras.layers import Conv2D, MaxPooling2D, AveragePooling2D
from keras.layers.normalization import BatchNormalization
from tensorflow.keras.optimizers import Adam
from tensorflow.keras import optimizers
from keras.preprocessing.image import ImageDataGenerator
from sklearn.model_selection import train_test_split

num_classes = 2

#Generating DataSet
path_root = "/content/dataset"
batches = ImageDataGenerator().flow_from_directory(directory=path_root, target_size=(224,224), batch_size=10000)
imgs, labels = next(batches)

#Split into train and test
X_train, X_test, y_train, y_test = train_test_split(imgs/255.,labels, test_size=0.2)

import efficientnet.keras as efn

base_model = efn.EfficientNetB0(input_shape = (224, 224, 3), include_top = False, weights = 'imagenet')

for layer in base_model.layers[:220]:
    layer.trainable = False

from keras import layers
x = base_model.output
x = Flatten()(x)
x = Dense(1024, activation="relu")(x)
x = Dropout(0.2)(x)
x = layers.Dense(num_classes, activation='softmax')(x)
model_final = keras.models.Model(base_model.input, x)

!pip install keras_metrics

import keras_metrics

opt = Adam(lr=0.000001)
#opt = Adam(lr=0.0000001)
#opt = optimizers.RMSprop(lr=0.000001)
model_final.compile(loss='categorical_crossentropy', optimizer = opt, metrics=['accuracy', keras_metrics.precision(), keras_metrics.binary_f1_score(), keras_metrics.recall()])

import numpy as np
y_train_new = np.argmax(y_train, axis=1)

from sklearn.utils import class_weight

y_train_new = np.argmax(y_train, axis=1)
#Deal with unbalanced Data
class_weights = class_weight.compute_class_weight('balanced', np.unique(y_train_new),y_train_new)
class_weights = dict(enumerate(class_weights))
class_weights

history = model_final.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=20, class_weight=class_weights)

scores = model_final.evaluate(X_test, y_test)



history = model_final.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=20)

scores = model_final.evaluate(X_test, y_test)

# Affiche de la courbe de l'accuracy et de la validation
import matplotlib.pyplot as plt

# Affichage des courbes d'accuracy, validation loss
epochs = range(len(history.history['accuracy']))
plt.plot(epochs, history.history['loss'], 'b', label='loss')
plt.plot(epochs, history.history['val_loss'], 'r', label='Val loss')
plt.title("Courbe d'accuracy de l'entrainement et de la validation loss ")
plt.legend()

plt.show()

# Affiche de la courbe de l'accuracy et de la validation

plt.plot(epochs, history.history['accuracy'], 'b', label='accuray')
plt.plot(epochs, history.history['val_accuracy'], 'r', label='Val acc')
plt.title("Courbe d'accuracy de l'entrainement et de la validation ")
plt.legend()
plt.figure()



#y_pred = model.predict_classes(X_test, verbose=0)
y_pred = np.argmax(model_final.predict(X_test), axis=-1)

y_test2 = np.argmax(y_test, axis=1)

from sklearn import metrics
c_matrix = metrics.confusion_matrix(y_test2, y_pred)

class_names= batches.class_indices.keys()

#retourne tous les labels de test
 import pandas as pd
# tableau a deux dimension de la matrice de confusion
 df_cm = pd.DataFrame(c_matrix, index=class_names, columns=class_names)
 df_cm

import seaborn as sns

plt.figure(figsize = (20,7))

sns.heatmap(df_cm, annot=True, fmt="d")



"""# References

1. https://keras.io/examples/vision/image_classification_efficientnet_fine_tuning/#:~:text=EfficientNet%2C%20first%20introduced%20in%20Tan,image%20classification%20transfer%20learning%20tasks.

2. https://medium.com/analytics-vidhya/image-classification-with-efficientnet-better-performance-with-computational-efficiency-f480fdb00ac6
"""

